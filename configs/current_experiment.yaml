data_arguments:
  dataset: "cvdb"
  block_size: 32
  label_block_size: 8
  train_subset: 'full'
  num_ents: 4000

model_arguments:
  seq2seq: True
  max_new_tokens: 8
  # config_name: "gpt2"
  # config_name: "EleutherAI/pythia-70m"
  config_name: "t5-small"
  # config_name: "EleutherAI/pythia-160m"
  separate_token_per_var: False # only used for numeric experiments


training_arguments:
  output_dir: 'experiments/temp'
  bf16: True
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  num_train_epochs: 20
  optim: "adamw_hf"
  overwrite_output_dir: True
  auto_find_batch_size: True
  save_strategy: "no"
  load_best_model_at_end: False
  evaluation_strategy: 'epoch'
  do_train: True
  do_eval: True
  do_sweeps: False
  n_sweeps: 5
  save_each_epochs: 0
  eval_each_epochs: 1
  eval_steps: 50
  eval_callback_type: "generate"  # pipeline or generate
  weight_decay: 0.0001


experiment_arguments: # common experiment arguments
  define_experiment: False
  numeric_experiment: True
  name_prefix: ""
  n_stages: 1
  n_seeds: 1
  n_seeds_stage2: 1
  start_seed: 67
  slurm: False


define_experiment_arguments:
  def_order: "tve"


numeric_experiment_arguments:
  modular_experiment_baseline: False
  modular_experiment: False
  num_choice_experiment: True
  num_x: 4000
  n_nums_in_question: 4
  n_intersecton: 1
  n_qs_per_x: 40
  p_label_flip: 0.0


# # overrides specified parameters
# first_stage_arguments:
#   train_subset: 'stage1'
#   num_train_epochs: 20
#   gradient_accumulation_steps: 1

# second_stage_arguments:
#   train_subset: 'stage2'
#   num_train_epochs: 5
#   gradient_accumulation_steps: 1
#   dont_save_in_the_end: True
#   save_each_epochs: 0

sweep_arguments:
  method: 'random'
  metric:
    name: 'eval_accuracy' # doesn't really matter, this parameter is further defined by compute objective trainer.hyp_search
    goal: 'maximize'
  parameters:
    optim:
      values: ['adamw_hf']
    per_device_train_batch_size:
      values: [512, 1024, 2048, 4096]
    # learning_rate:
    #   distribution: 'log_uniform_values'
    #   min: 0.0001
    #   max: 0.001